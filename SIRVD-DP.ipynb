{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SIRVD-DP.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOKO7QARVRVZZOdsbAgNRLj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"m5ndeeEHmja1"},"source":["# SIRVD-DP: A COVID-19 prediction model of deep learning based on time-dependant SIRVD"]},{"cell_type":"code","metadata":{"id":"_pkUbvHTmfmu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623282291341,"user_tz":-480,"elapsed":25002,"user":{"displayName":"兰鹏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwRFmqmOp0MznK8X7syJ-VSK5QtsNJXSsMiMy=s64","userId":"13969222230977378150"}},"outputId":"a62f7696-01f9-4aca-e005-a178f03b5903"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","google_dir =  \"/content/drive/MyDrive/\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"smPY4jOwM5Yx"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import datetime\n","from scipy.optimize import leastsq \n","import scipy as sp   \n","import math\n","import os\n","import re\n","from sklearn.metrics import mean_squared_error, r2_score\n","import numpy as np \n","\n","%matplotlib inline\n","plt.rcParams['font.sans-serif']=['SimHei'] \n","plt.rcParams['axes.unicode_minus']=False \n","uk = 'United Kingdom'\n","ch = 'China'\n","it = 'Italy'\n","fr = 'France'\n","sp = 'Spain'\n","ge = 'Germany'\n","ko = 'Korea, South'\n","br = 'Brazil'\n","india = 'India'\n","us = 'US'\n","\n","country_num = {ch: 1439323774, india: 1380004385, us: 331002647, uk: 67886004, it: 60461828, fr: 67564251, sp: 46754783, ge: 83783945,\n","               ko: 51269183, br: 212559409}\n","google_dir =  \"/content/drive/MyDrive/\"\n","modelSavePath = google_dir + \"model/\"\n","figSavePath = google_dir + \"figure/\"\n","confirmDataPath = google_dir + \"confirmedGlobal.csv\"\n","deathDataPath = google_dir + \"deathGlobal.csv\"\n","recoveredDataPath = google_dir + \"recoveredGlobal.csv\"\n","confirmUSDataPath = google_dir + \"confirmedUS.csv\"\n","deathUSDataPath = google_dir + \"deathUS.csv\"\n","recoveredUSDataPath = google_dir + \"recoveredUS.csv\"\n","# download\n","import requests\n","confirmedGlobalUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n","deathGlobalUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\"\n","recoveredGlobalUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv\"\n","confirmedUSUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv\"\n","deathUSUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv\"\n","recoveredUSUrl = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_US.csv\"\n","vaccinationUrl = \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv\"\n","vaccinationDataPath = google_dir + \"vaccinations.csv\"\n","\n","def downloadCsv(url, fileName):    \n","    r = requests.get(url)\n","    with open(fileName,'wb') as f:\n","        f.write(r.content)\n","        print(\"download completed\")\n","downloadCsv(confirmedGlobalUrl, confirmDataPath)\n","downloadCsv(deathGlobalUrl, deathDataPath)\n","downloadCsv(recoveredGlobalUrl, recoveredDataPath)\n","downloadCsv(confirmedUSUrl, confirmUSDataPath)\n","downloadCsv(deathUSUrl, deathUSDataPath)\n","downloadCsv(recoveredUSUrl, recoveredUSDataPath)\n","downloadCsv(vaccinationUrl, vaccinationDataPath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-OaiTIE5cwJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623282309443,"user_tz":-480,"elapsed":2528,"user":{"displayName":"兰鹏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwRFmqmOp0MznK8X7syJ-VSK5QtsNJXSsMiMy=s64","userId":"13969222230977378150"}},"outputId":"c4c46777-2872-4f0e-f45d-d5eb4eea6ed9"},"source":["# read data\n","def readDeathDataByCountry(countryName):\n","    death_data = pd.read_csv(deathDataPath)\n","    data = death_data[(death_data['Country/Region'] == countryName)].iloc[:,4:]\n","    # & (death_data['Province/State'].isna())\n","    return data.sum()\n","def readConfirmedDataByCountry(countryName):\n","    recover_data = pd.read_csv(confirmDataPath)\n","    data = recover_data[(recover_data['Country/Region'] == countryName)].iloc[:,4:]\n","    return data.sum()\n","def readRecoveredDataByCountry(countryName):\n","    recover_data = pd.read_csv(recoveredDataPath)\n","    data = recover_data[(recover_data['Country/Region'] == countryName)].iloc[:,4:]\n","    return data.sum()\n","def load_IR(name, N, t,T=None):\n","    if name == 'sars_bj':\n","        sars_bj = pd.read_excel('sars_bj.xlsx')\n","        R = sars_bj['R']\n","        D = sars_bj['D']\n","        C = sars_bj['C']\n","        if T == None:\n","          T = len(R)-1\n","        R = R.tolist()\n","        D = D.tolist()\n","        C = C.tolist()\n","    else:\n","        R = readRecoveredDataByCountry(name)\n","        D = readDeathDataByCountry(name)\n","        C = readConfirmedDataByCountry(name) \n","        if T == None:\n","          T = len(R)-1\n","        R = R.tolist()[t:T+1]\n","        D = D.tolist()[t:T+1]\n","        C = C.tolist()[t:T+1]\n","    R = np.array(R)\n","    D = np.array(D)\n","    C = np.array(C)\n","    I = C - R - D\n","    datestart=datetime.datetime.strptime(\"2020-1-22\",'%Y-%m-%d')\n","    datestart += datetime.timedelta(days=+t)\n","    dateend = datetime.datetime.strptime(\"2020-1-22\",'%Y-%m-%d') \n","    dateend += datetime.timedelta(days=T)\n","    print(\"load \" + name + \" IRDC from \", datestart.strftime('%Y-%m-%d'), \" to \", dateend.strftime('%Y-%m-%d'), \", total: \", len(I))\n","    return I,R,D,C\n","def create_assist_date(datestart = None,dateend = None):\n","    if datestart is None:\n","        datestart = '2016-01-01'\n","    if dateend is None:\n","        dateend = datetime.datetime.now().strftime('%Y-%m-%d')\n","    datestart=datetime.datetime.strptime(datestart,'%Y-%m-%d')\n","    dateend=datetime.datetime.strptime(dateend,'%Y-%m-%d')\n","    date_list = []\n","    date_list.append(datestart.strftime('%Y-%m-%d'))\n","    while datestart<dateend:\n","        datestart+=datetime.timedelta(days=+1)\n","        date_list.append(datestart.strftime('%Y-%m-%d'))\n","    return date_list\n","\n","def predictPlot(I, I_t):\n","    date_X = create_assist_date(\"2020-1-22\", \"2021-10-01\")\n","    X = np.arange(0, len(I_t))\n","    ax  = plt.figure(figsize=(13, 8))\n","    sns.lineplot(X[:len(I_t)],I_t,label=\"Predict Infected\")\n","    sns.lineplot(X[:len(I)], I, label = 'Current Infected')\n","    plt.xlabel('Date')\n","    plt.ylabel('Number of active infections')\n","    plt.title('SIR Model')\n","    \n","def plot(data,label=None):\n","    X = np.arange(0, len(data))\n","    ax  = plt.figure(figsize=(13, 8))\n","    sns.lineplot(X[:len(data)],data,label=label)\n","\n","def readVaccinationDataByCountry(countryName):\n","  vaccinationData = pd.read_csv(vaccinationDataPath)\n","  vacData = vaccinationData[vaccinationData['location'] == countryName]\n","  datestart = datetime.datetime.strptime(vacData['date'].iloc[0],'%Y-%m-%d')\n","  dateend = datetime.datetime.strptime(vacData['date'].iloc[-1],'%Y-%m-%d')\n","  startDate = datetime.datetime.strptime(\"2020-1-22\",'%Y-%m-%d')\n","  t = (datestart - startDate).days\n","  T = (dateend - startDate).days\n","  print(\"load vaccinatin data: \", countryName, \", start: \", datestart, \", end: \", dateend, \", total: \", (dateend - datestart).days+1, \", t: \", t, \" T: \", T)\n","  V = vacData['people_vaccinated'].tolist()\n","  V = np.array(V)\n","  for i in range(0, len(V)):\n","    if np.isnan(V[i]):\n","      V[i] = V[i-1]\n","  return V,t,T\n","india_V,t,T = readVaccinationDataByCountry(\"India\")\n","ch_data = load_IR(ch, country_num[ch], 0)\n","us_data = load_IR(us, country_num[us], 0)\n","india_data = load_IR(india, country_num[india], 0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["load vaccinatin data:  India , start:  2021-01-15 00:00:00 , end:  2021-05-27 00:00:00 , total:  133 , t:  359  T:  491\n","load China IRDC from  2020-01-22  to  2021-06-01 , total:  497\n","load US IRDC from  2020-01-22  to  2021-06-01 , total:  497\n","load India IRDC from  2020-01-22  to  2021-06-01 , total:  497\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZW7r4rQhAoRs","executionInfo":{"status":"ok","timestamp":1623282314740,"user_tz":-480,"elapsed":339,"user":{"displayName":"兰鹏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwRFmqmOp0MznK8X7syJ-VSK5QtsNJXSsMiMy=s64","userId":"13969222230977378150"}},"outputId":"3f8443ac-1d51-4c4d-bf4b-8cb8d597cf75"},"source":["def IRDCV(country):\n","  V,t,T = readVaccinationDataByCountry(country)\n","  I,R,D,C = load_IR(country, country_num[country], t, T)\n","  N = country_num[country]\n","  S = N - I - R - D - V\n","  #ax = plt.figure(figsize=(20, 15))\n","  #plt.plot(I, label='I')\n","  #plt.plot(R, label='R')\n","  #plt.plot(D, label='D')\n","  #plt.plot(C, label='C')\n","  #plt.plot(V, label='V')\n","  #plt.legend(loc='best')\n","\n","  # S->V: alpha\n","  # I->D: delta\n","  # I->R: gamma\n","  # S->I: beta\n","  # R->S: sigma = 0\n","  alpha = np.zeros(len(I)-1)\n","  delta = np.zeros(len(I)-1)\n","  gamma = np.zeros(len(I)-1)\n","  beta = np.zeros(len(I)-1)\n","  sigma = np.zeros(len(I)-1)\n","  for i in range(0, len(I)-1):\n","    alpha[i] = (V[i+1] - V[i]) / S[i]\n","    delta[i] = (D[i+1] - D[i]) / I[i]\n","    gamma[i] = (R[i+1] - R[i]) / I[i]\n","    beta[i] = (I[i+1] - I[i] + R[i+1] - R[i] + D[i+1] - D[i]) * N / (I[i] * S[i])\n","    sigma[i] = (I[i+1] - I[i] + R[i+1] - R[i] + D[i+1] - D[i] + S[i+1] - S[i] + V[i+1] - V[i]) / R[i]\n","  return S,I,R,V,D,C,alpha, delta, gamma, beta, sigma\n","  \n","def normalization(data):\n","  new_data = data.astype('float32')\n","  max_value = np.max(new_data)\n","  min_value = np.min(new_data)\n","  scalar = max_value - min_value\n","  new_data = list(map(lambda x: np.array((x) / scalar), new_data))\n","  return np.array(new_data), scalar\n","S,I,R,V,D,C,alpha, delta, gamma, beta, sigma = IRDCV(india)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["load vaccinatin data:  India , start:  2021-01-15 00:00:00 , end:  2021-05-27 00:00:00 , total:  133 , t:  359  T:  491\n","load India IRDC from  2021-01-15  to  2021-05-27 , total:  133\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U8yGG01iAyAm","executionInfo":{"status":"ok","timestamp":1623122575893,"user_tz":-480,"elapsed":290,"user":{"displayName":"兰鹏","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgHwRFmqmOp0MznK8X7syJ-VSK5QtsNJXSsMiMy=s64","userId":"13969222230977378150"}},"outputId":"7861ead5-d5b4-4e17-d709-83454e5349ae"},"source":["\n","S,I,R,V,D,alpha, delta, gamma, beta, sigma = IRDCV(india)\n","S, scalar_S = normalization(S)\n","I, scalar_I = normalization(I)\n","R, scalar_R = normalization(R)\n","V, scalar_V = normalization(V)\n","D, scalar_D = normalization(D)\n","alpha, scalar_alpha = normalization(alpha)\n","beta, scalar_beta = normalization(beta)\n","gamma, scalar_gamma = normalization(gamma)\n","delta, scalar_delta = normalization(delta)\n","T = len(beta)\n","window = 3\n","n = 1\n","days = 1\n","dataX, dataY = [], []\n","print(\"dataset length: \", T, \" window size: \", window, \" prediction days: \", days, \" feature num: \", n)\n","for i in range(T - window - days):\n","  matrix = []\n","  # matrix.append(S[i:(i+window)])\n","  # matrix.append(I[i:(i+window)])\n","  # matrix.append(R[i:(i+window)])\n","  # matrix.append(V[i:(i+window)])\n","  # matrix.append(D[i:(i+window)])\n","  matrix.append(beta[i:(i+window)])\n","  # matrix.append(gamma[i:(i+window)])\n","  # matrix.append(alpha[i:(i+window)])\n","  # matrix.append(delta[i:(i+window)])\n","  n = len(matrix)\n","  dataX.append(matrix)\n","  dataY.append(beta[(i + window):(i+window+days)])\n","dataX = np.array(dataX)\n","dataY = np.array(dataY)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["load vaccinatin data:  India , start:  2021-01-15 00:00:00 , end:  2021-05-27 00:00:00 , total:  133 , t:  359  T:  491\n","load India IRDC from  2021-01-15  to  2021-05-27 , total:  133\n","数据集总长度:  132  窗口大小:  3  预测天数:  1  特征数量:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zF28lNAIjjkI"},"source":["def createDataset(data, window=2, days=1):\n","  dataset = data\n","  dataset = dataset.astype('float32')\n","  max_value = np.max(dataset)\n","  min_value = np.min(dataset)\n","  scalar = max_value - min_value\n","  dataset = list(map(lambda x: np.array(x / scalar), dataset))\n","  dataX, dataY = [], []\n","  for i in range(len(dataset) - window - days):\n","      a = dataset[i:(i + window)]\n","      dataX.append(a)\n","      a = dataset[(i + window):(i+window+days)]\n","      dataY.append(a)\n","  return np.array(dataX), np.array(dataY), scalar\n","def splitTrainAndTest(data_X, data_Y, ratio=0.7):\n","  window = data_X.shape[2]\n","  days = data_Y.shape[1]\n","  train_size = int(len(data_X) * ratio)\n","  train_X = data_X[:train_size]\n","  train_Y = data_Y[:train_size]\n","  test_X = data_X[train_size:]\n","  test_Y = data_Y[train_size:]\n","  return train_X, train_Y, test_X, test_Y, train_size\n","def mean_absolute_percentage_error(y_true, y_pred): \n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","def printScore(y_true, y_pred):\n","  if y_true.shape != y_pred.shape:\n","    y_pred = y_pred.reshape(-1,1)\n","  print(f\"(MSE)：{mean_squared_error(y_true, y_pred)}\")\n","  print(f\"(RMSE)：{np.sqrt(mean_squared_error(y_true, y_pred))}\")\n","  print(f\"(NRMSE)：{np.sqrt(mean_squared_error(y_true, y_pred))/y_true.mean()}\")\n","  print(f\"R^2：{r2_score(y_true, y_pred)}\")\n","  print(f\"(MAPE)：{mean_absolute_percentage_error(y_true, y_pred)}\")\n","\n","def computeSmape(A, F):\n","    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))\n","\n","def getScore(y_true, y_pred):\n","  if y_true.shape != y_pred.shape:\n","    y_pred = y_pred.reshape(-1,1)\n","  mse = mean_squared_error(y_true, y_pred)\n","  rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n","  nrmse = np.sqrt(mean_squared_error(y_true, y_pred))/y_true.mean()\n","  r2 = r2_score(y_true, y_pred)\n","  mape = mean_absolute_percentage_error(y_true, y_pred)\n","  return (mse, rmse,nrmse, r2, mape)\n","\n","def plotComparison(real, predict):\n","  if real.shape != predict.shape:\n","    predict = predict.reshape(-1,1)\n","  ax = plt.figure(figsize=(20, 15))\n","  plt.plot(predict, 'r', label='prediction')\n","  plt.plot(real, 'b', label='real')\n","  plt.legend(loc='best')\n","def plotComparisonScatter(real, predict):\n","  if real.shape != predict.shape:\n","    predict = predict.reshape(-1,1)\n","  ax = plt.figure(figsize=(20, 15))\n","  plt.plot(np.arange(0,len(predict),1), predict, 'o',color = 'r', label='prediction')\n","  plt.plot(np.arange(0,len(real),1), real, 'o','b', label='real')\n","  plt.legend(loc='best')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EctswKIDIXlc"},"source":["from tensorflow.keras.layers import Dense, Input, Conv1D, Conv2D, GlobalAveragePooling2D, GlobalAveragePooling1D\n","from tensorflow.keras.initializers import he_normal\n","from tensorflow.keras.models import Model, load_model\n","from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adagrad\n","from tensorflow import keras\n","import sys\n","import os\n","from tensorflow.compat.v1 import ConfigProto\n","from tensorflow.compat.v1 import InteractiveSession\n","from keras.datasets import mnist\n","from keras.layers import Dense, LSTM, Bidirectional, Conv1D, Conv2D, MaxPooling1D, MaxPool2D, Flatten\n","from keras.layers.wrappers import Bidirectional\n","#from keras.utils import to_categorical\n","from keras.models import Sequential\n","import tensorflow as tf\n","from numpy.random import seed\n","seed(7)\n","\n","def VanillaLstm(input_dim, window_size, output_dim, optimizer='adam', learning_rate=0.01):\n","  #parameters for LSTM\n","  nb_lstm_outputs = 16  \n","  #build model\n","  model = Sequential()\n","  model.add(LSTM(units=nb_lstm_outputs, input_shape=[input_dim, window_size]))\n","  model.add(Dense(units=output_dim,\n","                  input_dim=input_dim,\n","                  activation='linear'))\n","  # 'sgd', 'rmsprop', 'adam', 'adagrad'\n","  if optimizer=='adam':\n","    optimizer = Adam(learning_rate=learning_rate)\n","  elif optimizer=='sgd':\n","    optimizer = SGD(learning_rate=learning_rate)\n","  elif optimizer=='rmsprop':\n","    optimizer = RMSprop(learning_rate=learning_rate)\n","  elif optimizer=='adagrad':\n","    optimizer = Adagrad(learning_rate=learning_rate)\n","  return model\n","\n","# Bi LSTM \n","def BiDirectionalLstm(input_dim, window_size, output_dim):\n","    model = Sequential()\n","    model.add(Bidirectional(LSTM(16, activation ='relu'), input_shape = [input_dim, window_size]))\n","    model.add(Dense(input_dim))\n","    model.add(Dense(output_dim))\n","    return model\n","\n","# Stacked-LSTM\n","def StackedLstm(input_dim, window_size, output_dim):\n","    model = keras.models.Sequential([\n","        keras.layers.LSTM(16, return_sequences=True, input_shape=[input_dim, window_size], activation='relu',\n","                          recurrent_activation='sigmoid'),\n","        keras.layers.LSTM(32, activation='relu', return_sequences=True, recurrent_activation='sigmoid'),\n","        keras.layers.LSTM(16, activation='relu', recurrent_activation='sigmoid'),\n","        Dense(output_dim)\n","    ])\n","    return model\n","\n","# GRU\n","def GRU(input_dim, window_size, output_dim):\n","    model = keras.models.Sequential([\n","        keras.layers.GRU(16, return_sequences=True, input_shape=[input_dim, window_size], activation='relu',\n","                         recurrent_activation='sigmoid', reset_after=True),\n","        keras.layers.GRU(32, activation='relu', return_sequences=True, recurrent_activation='sigmoid',\n","                         reset_after=True),\n","        keras.layers.GRU(16, activation='relu', recurrent_activation='sigmoid', reset_after=True),\n","        Dense(output_dim)\n","    ])\n","    return model\n","    \n","def testSingleModel(model, data, window, days, verbose=0):\n","  data_X, data_Y, scalar = createDataset(data, window, days)\n","  train_X, train_Y, test_X, test_Y = splitTrainAndTest(data_X, data_Y)\n","  print(\"train_X shape: \", train_X.shape)\n","  print(\"train_Y shape: \", train_Y.shape)\n","  model = model(window, days)\n","  model.compile(loss='mean_squared_error',optimizer='adam')\n","  model.fit(train_X, train_Y, epochs=530, batch_size=128, verbose=verbose)\n","  data_X = data_X.reshape(-1, 1, window)\n","  y_pred = model.predict(data_X)\n","  y_real = data_Y * scalar\n","  y_pred = y_pred * scalar\n","  printScore(y_real, y_pred)\n","  plotComparison(y_real, y_pred)\n","\n","class LossHistory(keras.callbacks.Callback):\n","  def on_train_begin(self, logs={}):\n","      self.losses = {'batch':[], 'epoch':[]}\n","      # self.accuracy = {'batch':[], 'epoch':[]}\n","      # self.val_loss = {'batch':[], 'epoch':[]}\n","      # self.val_acc = {'batch':[], 'epoch':[]}\n","\n","  def on_batch_end(self, batch, logs={}):\n","      self.losses['batch'].append(logs.get('loss'))\n","      # self.accuracy['batch'].append(logs.get('acc'))\n","      # self.val_loss['batch'].append(logs.get('val_loss'))\n","      # self.val_acc['batch'].append(logs.get('val_acc'))\n","\n","  def on_epoch_end(self, batch, logs={}):\n","      self.losses['epoch'].append(logs.get('loss'))\n","      # self.accuracy['epoch'].append(logs.get('acc'))\n","      # self.val_loss['epoch'].append(logs.get('val_loss'))\n","      # self.val_acc['epoch'].append(logs.get('val_acc'))\n","\n","  def loss_plot(self, loss_type):\n","      iters = range(len(self.losses[loss_type]))\n","      plt.figure()\n","      # acc\n","      #plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n","      # loss\n","      plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n","      # if loss_type == 'epoch':\n","      #     # val_acc\n","      #     plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n","      #     # val_loss\n","      #     plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n","      plt.grid(True)\n","      plt.xlabel(loss_type)\n","      plt.ylabel('acc-loss')\n","      plt.legend(loc=\"upper right\")\n","      plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nXZSY-pRfbFz"},"source":["history = LossHistory()\n","model = StackedLstm(n, window, days)\n","model.compile(loss='mean_squared_error',optimizer='adam')\n","train_X, train_Y, test_X, test_Y, trainSize = splitTrainAndTest(dataX, dataY)\n","model.fit(train_X, train_Y, epochs=2000, batch_size=128, verbose=1,callbacks=[history])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"APPtOhHtqz7e"},"source":["history.loss_plot('epoch')\n","print(\"train：\")\n","y_pred = model.predict(train_X)\n","y_real = train_Y * scalar_beta\n","y_pred = y_pred * scalar_beta\n","printScore(y_real, y_pred)\n","plotComparison(y_real, y_pred)\n","plotComparisonScatter(y_real, y_pred)\n","print(\"test\")\n","y_pred = model.predict(test_X)\n","y_real = test_Y * scalar_beta\n","y_pred = y_pred * scalar_beta\n","printScore(y_real, y_pred)\n","plotComparison(y_real, y_pred)\n","plotComparisonScatter(y_real, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-pqX02BZSa9s"},"source":["# SIRVD-DP\n","S,I,R,V,D,C,alpha, delta, gamma, beta, sigma = IRDCV(india)\n","S, scalar_S = normalization(S)\n","I, scalar_I = normalization(I)\n","R, scalar_R = normalization(R)\n","V, scalar_V = normalization(V)\n","D, scalar_D = normalization(D)\n","def moving_average(xt):\n","  for i in range(2, len(xt)-1):\n","    xt[i] = (xt[i-1] + xt[i])/2\n","  return xt\n","\n","alpha, scalar_alpha = normalization(alpha)\n","beta, scalar_beta = normalization(beta)\n","gamma, scalar_gamma = normalization(gamma)\n","delta, scalar_delta = normalization(delta)\n","\n","beta = moving_average(beta)\n","gamma = moving_average(gamma)\n","delta = moving_average(beta)\n","\n","T = len(beta)\n","window = 3\n","n = 1\n","days = 1\n","\n","def getFeatureXY(feature_str = \"I\", output_str = \"I\"):\n","  dataX, dataY = [], []\n","  for i in range(T - window - days):\n","    matrix = []\n","    if 'S' in feature_str:\n","      matrix.append(S[i:(i+window)])\n","    if 'I' in feature_str:\n","      matrix.append(I[i:(i+window)])\n","    if 'R' in feature_str:\n","      matrix.append(R[i:(i+window)])\n","    if 'V' in feature_str:\n","      matrix.append(V[i:(i+window)])\n","    if 'D' in feature_str:\n","      matrix.append(D[i:(i+window)])\n","    if 'b' in feature_str:\n","      matrix.append(beta[i:(i+window)])\n","    if 'g' in feature_str:\n","      matrix.append(gamma[i:(i+window)])\n","    if 'a' in feature_str:\n","      matrix.append(alpha[i:(i+window)])\n","    if 'd' in feature_str:\n","      matrix.append(delta[i:(i+window)])\n","    dataX.append(matrix)\n","    #\n","    if 'S' in output_str:\n","      dataY.append(S[(i + window):(i+window+days)])\n","    if 'I' in output_str:\n","      dataY.append(I[(i + window):(i+window+days)])\n","    if 'R' in output_str:\n","      dataY.append(R[(i + window):(i+window+days)])\n","    if 'V' in output_str:\n","      dataY.append(V[(i + window):(i+window+days)])\n","    if 'D' in output_str:\n","      dataY.append(D[(i + window):(i+window+days)])\n","    if 'b' in output_str:\n","      dataY.append(beta[(i + window):(i+window+days)])\n","    if 'g' in output_str:\n","      dataY.append(gamma[(i + window):(i+window+days)])\n","    if 'a' in output_str:\n","      dataY.append(alpha[(i + window):(i+window+days)])\n","    if 'd' in output_str:\n","      dataY.append(delta[(i + window):(i+window+days)])\n","  n = len(dataX[0])\n","  print(f\" input features: {feature_str} output feature: {output_str} total length: {T} window size: {window} prediction days: {days} feature nums: {n}\")\n","  dataX = np.array(dataX)\n","  dataY = np.array(dataY)\n","  return dataX, dataY\n","\n","dataX_S, dataY_S = getFeatureXY(\"S\",\"S\")\n","dataX_I, dataY_I = getFeatureXY(\"I\",\"I\")\n","dataX_R, dataY_R = getFeatureXY(\"R\",\"R\")\n","dataX_V, dataY_V = getFeatureXY(\"V\",\"V\")\n","dataX_D, dataY_D = getFeatureXY(\"D\",\"D\")\n","dataX_beta, dataY_beta = getFeatureXY(\"b\",\"b\")\n","dataX_gamma, dataY_gamma = getFeatureXY(\"g\",\"g\")\n","dataX_delta, dataY_delta = getFeatureXY(\"d\",\"d\")\n","dataX_alpha, dataY_alpha = getFeatureXY(\"a\",\"a\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LJgJqWxzVKBW"},"source":["model_set = {'VanillaLstm':VanillaLstm, \"BiDirectionalLstm\":BiDirectionalLstm, \"StackedLstm\":StackedLstm, \"GRU\":GRU}\n","def getPredictedParam(model, dataX, dataY, scalar, verbose=1):\n","  history = LossHistory()\n","  n = dataX[0].shape[0]\n","  model = model(n, window, days)\n","  model.compile(loss='mean_squared_error',optimizer='adam')\n","  train_X, train_Y, test_X, test_Y, start_size = splitTrainAndTest(dataX, dataY)\n","  train_start_time = datetime.datetime.now()\n","  print(f\"begin {train_start_time}\")\n","  model.fit(train_X, train_Y, epochs=3000, batch_size=128, verbose=0,callbacks=[history])\n","  total_time = datetime.datetime.now() - train_start_time\n","  print(f\"end {total_time}\")\n","  # history.loss_plot('epoch')\n","  \n","  y_pred = model.predict(train_X)\n","  y_train_real = train_Y * scalar\n","  y_pred = y_pred * scalar\n","  train_score = getScore(y_train_real, y_pred)\n","  if verbose == 1:\n","    plotComparison(y_train_real, y_pred)\n","    #plotComparisonScatter(y_real, y_pred)\n","  y_train_pred = y_pred\n","\n","  y_pred = model.predict(test_X)\n","  y_test_real = test_Y * scalar\n","  y_pred = y_pred * scalar\n","  test_score = getScore(y_test_real, y_pred)\n","  if verbose == 1:\n","    plotComparison(y_test_real, y_pred)\n","    #plotComparisonScatter(y_real, y_pred)\n","  y_test_pred = y_pred\n","  print(f\"train: {train_score} test: {test_score}\")\n","  return y_train_pred, y_test_pred, y_train_real, y_test_real, train_score, test_score\n","\n","def getParamPredictedResult(dataX_param, dataY_param, scalar_param):\n","  model_result = []\n","  model_train_score = []\n","  model_test_score = []\n","  for i in range(10):\n","    for model in model_set:\n","      beta_train_pred, beta_test_pred, beta_train_real, beta_test_real, beta_train_score, beta_test_score = getPredictedParam(model_set[model], dataX_param, dataY_param, scalar_param)\n","      model_result.append((beta_train_pred, beta_test_pred, beta_train_real, beta_test_real))\n","      train_score = list(beta_train_score)\n","      train_score.insert(0, model)\n","      model_train_score.append(train_score)\n","      test_score = list(beta_test_score)\n","      test_score.insert(0, model)\n","      model_test_score.append(test_score)\n","  return (model_result, model_train_score, model_test_score)\n","\n","model_result = []\n","model_train_score = []\n","model_test_score = []\n","for i in range(10):\n","  for model in model_set:\n","    beta_train_pred, beta_test_pred, beta_train_real, beta_test_real, beta_train_score, beta_test_score = getPredictedParam(model_set[model], dataX_beta, dataY_beta, scalar_beta)\n","    model_result.append((beta_train_pred, beta_test_pred, beta_train_real, beta_test_real))\n","    train_score = list(beta_train_score)\n","    train_score.insert(0, model)\n","    model_train_score.append(train_score)\n","    test_score = list(beta_test_score)\n","    test_score.insert(0, model)\n","    model_test_score.append(test_score)\n","param_model = getParamPredictedResult(dataX_alpha, dataY_alpha, scalar_alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m62vzsodvgYz"},"source":["beta_model = (model_result, model_train_score, model_test_score)\n","df2 = open(modelSavePath + \"delta_model.pickle\",'wb')\n","pickle.dump(param_model,df2)\n","df2.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkZKjR0kSuHz"},"source":["import pickle\n","df_beta = open(modelSavePath + \"beta_model.pickle\",'rb')\n","beta_model = pickle.load(df_beta)\n","df_beta.close()\n","df_gamma = open(modelSavePath + \"gamma_model.pickle\",'rb')\n","gamma_model = pickle.load(df_gamma)\n","df_gamma.close()\n","df_delta = open(modelSavePath + \"delta_model.pickle\",'rb')\n","delta_model = pickle.load(df_delta)\n","df_delta.close()\n","dataY_I_real = dataY_I * scalar_I \n","dataY_S_real = dataY_S * scalar_S\n","dataY_beta_real = dataY_beta * scalar_beta\n","dataY_gamma_real = dataY_gamma * scalar_gamma \n","dataY_delta_real = dataY_delta * scalar_delta "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhCBrnzLTH05"},"source":["SIRDP_train_result = []\n","SIRDP_test_result = []\n","for i in range(len(beta_model[0])):\n","  beta_train_pred, beta_test_pred, beta_train_real, beta_test_real = beta_model[0][i]\n","  gamma_train_pred, gamma_test_pred, gamma_train_real, gamma_test_real = gamma_model[0][i]\n","  delta_train_pred, delta_test_pred, delta_train_real, delta_test_real = delta_model[0][i]\n","  start_size = int(len(dataY_beta_real) * 0.7)\n","  pred_I = []\n","  for t in range(start_size, len(dataY_I_real)):\n","    predI = dataY_I_real[t-1] * ( 1 + beta_test_pred[t-start_size-1] * dataY_S_real[t-1] / country_num[india] - gamma_test_pred[t-start_size-1] - delta_test_pred[t-start_size-1])\n","    pred_I.append(predI)\n","  y_real = dataY_I_real[start_size+1:]\n","  y_pred = np.array(pred_I[1:])\n","  test_result = getScore(y_real, y_pred)\n","  SIRDP_test_result.append(test_result)\n","\n","  train_pred_I = [0]\n","  for t in range(1, len(beta_train_pred)):\n","    predI = dataY_I_real[t-1] * ( 1 + beta_train_pred[t-1] * dataY_S_real[t-1] / country_num[india] - gamma_train_pred[t-1] - delta_train_pred[t-1])\n","    train_pred_I.append(predI)\n","  y_real = dataY_I_real[1:start_size]\n","  y_pred = np.array(train_pred_I[1:])\n","  train_result = getScore(y_real, y_pred)\n","  SIRDP_train_result.append(train_result)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JECKsZ6GiQ-F"},"source":["df_SIRDP_train = pd.DataFrame(SIRDP_train_result, columns=[\"mse\",\"rmse\",\"nrmse\",\"r2\",\"mape\"])\n","df_SIRDP_test = pd.DataFrame(SIRDP_test_result, columns=[\"mse\",\"rmse\",\"nrmse\",\"r2\",\"mape\"])\n","df_SIRDP_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kooZGIRmvNzi"},"source":["beta_train_pred, beta_test_pred, beta_train_real, beta_test_real, beta_train_score, beta_test_score = getPredictedParam(StackedLstm, dataX_beta, dataY_beta, scalar_beta, verbose=1)\n","gamma_train_pred, gamma_test_pred, gamma_train_real, gamma_test_real, gamma_train_score, gamma_test_score = getPredictedParam(StackedLstm, dataX_gamma, dataY_gamma, scalar_gamma)\n","delta_train_pred, delta_test_pred, delta_train_real, delta_test_real, delta_train_score, delta_test_score = getPredictedParam(StackedLstm, dataX_delta, dataY_delta, scalar_delta)\n","dataY_I = dataY_I * scalar_I \n","dataY_S = dataY_S * scalar_S\n","dataY_beta = dataY_beta * scalar_beta\n","dataY_gamma = dataY_gamma * scalar_gamma \n","dataY_delta = dataY_delta * scalar_delta "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8iPR1HaSVOW"},"source":["start_size = int(len(dataY_beta) * 0.7)\n","\n","pred_I = []\n","for t in range(start_size, len(dataY_I)):\n","  predI = dataY_I[t-1] * ( 1 + beta_test_pred[t-start_size-1] * dataY_S[t-1] / country_num[india] - gamma_test_pred[t-start_size-1] - delta_test_pred[t-start_size-1])\n","  pred_I.append(predI)\n","y_real = dataY_I[start_size+1:]\n","y_pred = np.array(pred_I[1:])\n","print(\"test：\")\n","printScore(y_real, y_pred)\n","plotComparison(y_real, y_pred)\n","plotComparisonScatter(y_real, y_pred)\n","\n","train_pred_I = [0]\n","for t in range(1, len(beta_train_pred)):\n","  predI = dataY_I[t-1] * ( 1 + beta_train_pred[t-1] * dataY_S[t-1] / country_num[india] - gamma_train_pred[t-1] - delta_train_pred[t-1])\n","  train_pred_I.append(predI)\n","y_real = dataY_I[1:start_size]\n","y_pred = np.array(train_pred_I[1:])\n","print(\"train：\")\n","printScore(y_real, y_pred)\n","plotComparison(y_real, y_pred)\n","plotComparisonScatter(y_real, y_pred)\n"],"execution_count":null,"outputs":[]}]}